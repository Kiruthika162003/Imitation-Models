{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iGA1sofabbF",
        "outputId": "ec27324b-140a-4b14-85da-f1c128d784a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward for Agent 0: 0.9000000000000006\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SwarmAgent:\n",
        "    def __init__(self, agent_id, position):\n",
        "        self.agent_id = agent_id\n",
        "        self.position = position\n",
        "\n",
        "class MaxCausalIRL:\n",
        "    def __init__(self, num_agents, environment_size, num_iterations, learning_rate):\n",
        "        self.num_agents = num_agents\n",
        "        self.environment_size = environment_size\n",
        "        self.num_iterations = num_iterations\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.reward_map = np.zeros(environment_size)\n",
        "\n",
        "    def train(self, expert_trajectories):\n",
        "        for _ in range(self.num_iterations):\n",
        "            gradients = np.zeros(self.environment_size)\n",
        "\n",
        "            for trajectory in expert_trajectories:\n",
        "                agent_positions = [agent.position for agent in trajectory]\n",
        "\n",
        "                for agent in trajectory:\n",
        "                    expert_feature_expectation = self.calculate_feature_expectation(agent_positions)\n",
        "                    agent_feature_expectation = self.calculate_feature_expectation(agent_positions, agent.position)\n",
        "\n",
        "                    gradients += expert_feature_expectation - agent_feature_expectation\n",
        "\n",
        "            self.reward_map += self.learning_rate * gradients\n",
        "\n",
        "    def calculate_feature_expectation(self, agent_positions, position=None):\n",
        "        feature_expectation = np.zeros(self.environment_size)\n",
        "\n",
        "        for agent_position in agent_positions:\n",
        "            if position is None or (position[0] == agent_position[0] and position[1] == agent_position[1]):\n",
        "                feature_expectation[tuple(agent_position)] += 1\n",
        "\n",
        "        feature_expectation /= len(agent_positions)\n",
        "        return feature_expectation\n",
        "\n",
        "    def get_reward(self, agent):\n",
        "        reward = self.reward_map[tuple(agent.position)]\n",
        "        return reward\n",
        "\n",
        "# Usage example\n",
        "num_agents = 10\n",
        "environment_size = (10, 10)\n",
        "num_iterations = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "swarm_agents = []\n",
        "for i in range(num_agents):\n",
        "    position = np.random.randint(0, environment_size[0]), np.random.randint(0, environment_size[1])\n",
        "    agent = SwarmAgent(i, position)\n",
        "    swarm_agents.append(agent)\n",
        "\n",
        "expert_trajectories = [swarm_agents]  # List of expert agent trajectories\n",
        "\n",
        "maxcausal_irl = MaxCausalIRL(num_agents, environment_size, num_iterations, learning_rate)\n",
        "maxcausal_irl.train(expert_trajectories)\n",
        "\n",
        "# Get the reward for a specific agent\n",
        "agent_id = 0\n",
        "reward = maxcausal_irl.get_reward(swarm_agents[agent_id])\n",
        "print(f\"Reward for Agent {agent_id}: {reward}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code starts by defining a SwarmAgent class representing an agent in the swarm. Each agent has an agent ID and a position in the environment.\n",
        "\n",
        "The MaxCausalIRL class is defined to implement Maximum Causal Entropy Inverse Reinforcement Learning (MaxCausalIRL) in swarm systems. It takes the number of agents, environment size, number of iterations, and learning rate as input.\n",
        "\n",
        "The MaxCausalIRL class has a reward_map attribute initialized as an array of zeros with the size of the environment. This map will store the estimated rewards for different positions.\n",
        "\n",
        "The train method in the MaxCausalIRL class trains the reward model using MaxCausalIRL. It iterates for the specified number of iterations and calculates gradients by comparing the feature expectations of expert trajectories with agent trajectories.\n",
        "\n",
        "The calculate_feature_expectation method in the MaxCausalIRL class calculates the feature expectation given a list of agent positions. It counts the occurrences of each position and normalizes the counts.\n",
        "\n",
        "The get_reward method in the MaxCausalIRL class returns the estimated reward for a specific agent based on its position in the reward_map.\n",
        "\n",
        "In the usage example, a swarm of agents is created with random positions in the environment.\n",
        "\n",
        "Expert trajectories are defined as a list of agent trajectories. In this example, only a single expert trajectory is used.\n",
        "\n",
        "An instance of MaxCausalIRL is created, passing the number of agents, environment size, number of iterations, and learning rate.\n",
        "\n",
        "The train method is called to train the reward model using MaxCausalIRL with the expert trajectories.\n",
        "\n",
        "The get_reward method is called to retrieve the estimated reward for a specific agent, specified by its agent ID.\n",
        "\n",
        "The estimated reward for the agent is printed."
      ],
      "metadata": {
        "id": "I9Ptb2QNay6-"
      }
    }
  ]
}