{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6l-rpYBa-0M",
        "outputId": "ff7dbf02-bebc-4103-8646-175939cbd70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-62d2e131f6c3>:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  return torch.tensor(states, dtype=torch.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Discriminator Loss: 1.3538991808891296, Generator Loss: 0.7895625233650208\n",
            "Epoch: 10, Discriminator Loss: 1.162130892276764, Generator Loss: 0.7539633512496948\n",
            "Epoch: 20, Discriminator Loss: 1.039968490600586, Generator Loss: 0.7292135953903198\n",
            "Epoch: 30, Discriminator Loss: 0.9167066216468811, Generator Loss: 0.7773917317390442\n",
            "Epoch: 40, Discriminator Loss: 1.0213245451450348, Generator Loss: 0.6159793734550476\n",
            "Epoch: 50, Discriminator Loss: 0.7333255410194397, Generator Loss: 0.9421995878219604\n",
            "Epoch: 60, Discriminator Loss: 0.6393655091524124, Generator Loss: 1.162429690361023\n",
            "Epoch: 70, Discriminator Loss: 1.1317084431648254, Generator Loss: 0.6414823532104492\n",
            "Epoch: 80, Discriminator Loss: 1.2636996507644653, Generator Loss: 0.640957236289978\n",
            "Epoch: 90, Discriminator Loss: 1.5276044011116028, Generator Loss: 0.5240001678466797\n",
            "Epoch: 100, Discriminator Loss: 1.1340372562408447, Generator Loss: 0.9751554131507874\n",
            "Epoch: 110, Discriminator Loss: 1.4788191318511963, Generator Loss: 0.5873514413833618\n",
            "Epoch: 120, Discriminator Loss: 0.8526073396205902, Generator Loss: 1.4157521724700928\n",
            "Epoch: 130, Discriminator Loss: 1.5102596580982208, Generator Loss: 0.519023060798645\n",
            "Epoch: 140, Discriminator Loss: 0.8526828289031982, Generator Loss: 1.4113445281982422\n",
            "Epoch: 150, Discriminator Loss: 1.4817947149276733, Generator Loss: 0.5729875564575195\n",
            "Epoch: 160, Discriminator Loss: 1.3146534860134125, Generator Loss: 0.750153660774231\n",
            "Epoch: 170, Discriminator Loss: 0.9971308410167694, Generator Loss: 1.1296437978744507\n",
            "Epoch: 180, Discriminator Loss: 0.6818349957466125, Generator Loss: 1.6880860328674316\n",
            "Epoch: 190, Discriminator Loss: 0.5028540417551994, Generator Loss: 2.562300682067871\n",
            "Epoch: 200, Discriminator Loss: 0.4566894918680191, Generator Loss: 2.34084415435791\n",
            "Epoch: 210, Discriminator Loss: 0.5985181629657745, Generator Loss: 1.774644136428833\n",
            "Epoch: 220, Discriminator Loss: 3.0902277529239655, Generator Loss: 0.11451654136180878\n",
            "Epoch: 230, Discriminator Loss: 0.6288643628358841, Generator Loss: 2.426907539367676\n",
            "Epoch: 240, Discriminator Loss: 0.9315468966960907, Generator Loss: 1.3841257095336914\n",
            "Epoch: 250, Discriminator Loss: 1.4134891033172607, Generator Loss: 0.7854371666908264\n",
            "Epoch: 260, Discriminator Loss: 0.6984719187021255, Generator Loss: 2.021955966949463\n",
            "Epoch: 270, Discriminator Loss: 1.061177372932434, Generator Loss: 1.0149731636047363\n",
            "Epoch: 280, Discriminator Loss: 0.5153254494071007, Generator Loss: 2.769721746444702\n",
            "Epoch: 290, Discriminator Loss: 0.8466840088367462, Generator Loss: 1.4922127723693848\n",
            "Epoch: 300, Discriminator Loss: 1.3216909766197205, Generator Loss: 0.8575960993766785\n",
            "Epoch: 310, Discriminator Loss: 0.8596316874027252, Generator Loss: 1.5247687101364136\n",
            "Epoch: 320, Discriminator Loss: 1.8886192440986633, Generator Loss: 0.39061135053634644\n",
            "Epoch: 330, Discriminator Loss: 2.3987886905670166, Generator Loss: 0.35774296522140503\n",
            "Epoch: 340, Discriminator Loss: 2.87967711687088, Generator Loss: 0.17626692354679108\n",
            "Epoch: 350, Discriminator Loss: 1.403659701347351, Generator Loss: 0.7917196154594421\n",
            "Epoch: 360, Discriminator Loss: 2.4359882175922394, Generator Loss: 0.2596396803855896\n",
            "Epoch: 370, Discriminator Loss: 0.5944705903530121, Generator Loss: 2.384791612625122\n",
            "Epoch: 380, Discriminator Loss: 0.7110147774219513, Generator Loss: 2.0527379512786865\n",
            "Epoch: 390, Discriminator Loss: 0.6056573837995529, Generator Loss: 2.185190200805664\n",
            "Epoch: 400, Discriminator Loss: 0.8954754769802094, Generator Loss: 1.560290813446045\n",
            "Epoch: 410, Discriminator Loss: 0.4179109912365675, Generator Loss: 4.319777488708496\n",
            "Epoch: 420, Discriminator Loss: 3.104025572538376, Generator Loss: 0.1250983476638794\n",
            "Epoch: 430, Discriminator Loss: 0.6610896289348602, Generator Loss: 2.0747480392456055\n",
            "Epoch: 440, Discriminator Loss: 0.41721375472843647, Generator Loss: 5.195603847503662\n",
            "Epoch: 450, Discriminator Loss: 0.369499072432518, Generator Loss: 3.2781260013580322\n",
            "Epoch: 460, Discriminator Loss: 0.48483873903751373, Generator Loss: 2.7733800411224365\n",
            "Epoch: 470, Discriminator Loss: 0.42721499502658844, Generator Loss: 3.278964042663574\n",
            "Epoch: 480, Discriminator Loss: 0.4911581575870514, Generator Loss: 2.510986566543579\n",
            "Epoch: 490, Discriminator Loss: 1.1471876800060272, Generator Loss: 0.9089968800544739\n",
            "Epoch: 500, Discriminator Loss: 1.3851966857910156, Generator Loss: 0.7596136927604675\n",
            "Epoch: 510, Discriminator Loss: 0.48999667167663574, Generator Loss: 2.5064828395843506\n",
            "Epoch: 520, Discriminator Loss: 2.5493085980415344, Generator Loss: 0.3200004994869232\n",
            "Epoch: 530, Discriminator Loss: 0.6880366504192352, Generator Loss: 1.7335113286972046\n",
            "Epoch: 540, Discriminator Loss: 2.0771871209144592, Generator Loss: 0.3924315273761749\n",
            "Epoch: 550, Discriminator Loss: 0.4862948879599571, Generator Loss: 2.6003305912017822\n",
            "Epoch: 560, Discriminator Loss: 0.2906117991078645, Generator Loss: 6.0212626457214355\n",
            "Epoch: 570, Discriminator Loss: 0.2954478906467557, Generator Loss: 5.65590763092041\n",
            "Epoch: 580, Discriminator Loss: 0.2840159013867378, Generator Loss: 4.036007404327393\n",
            "Epoch: 590, Discriminator Loss: 1.6255291998386383, Generator Loss: 0.8237850069999695\n",
            "Epoch: 600, Discriminator Loss: 0.46726247668266296, Generator Loss: 2.9234812259674072\n",
            "Epoch: 610, Discriminator Loss: 0.4175228774547577, Generator Loss: 2.1188461780548096\n",
            "Epoch: 620, Discriminator Loss: 3.38625630736351, Generator Loss: 0.08943875879049301\n",
            "Epoch: 630, Discriminator Loss: 0.3733211988583207, Generator Loss: 4.966581344604492\n",
            "Epoch: 640, Discriminator Loss: 1.6293253004550934, Generator Loss: 0.5064247250556946\n",
            "Epoch: 650, Discriminator Loss: 1.9787289798259735, Generator Loss: 0.3961065113544464\n",
            "Epoch: 660, Discriminator Loss: 0.5689132064580917, Generator Loss: 2.2914791107177734\n",
            "Epoch: 670, Discriminator Loss: 0.41320808604359627, Generator Loss: 3.767009735107422\n",
            "Epoch: 680, Discriminator Loss: 1.4229044914245605, Generator Loss: 0.6538523435592651\n",
            "Epoch: 690, Discriminator Loss: 0.46919240802526474, Generator Loss: 3.591517210006714\n",
            "Epoch: 700, Discriminator Loss: 2.5413666367530823, Generator Loss: 0.1891661286354065\n",
            "Epoch: 710, Discriminator Loss: 0.3860813518986106, Generator Loss: 4.909702777862549\n",
            "Epoch: 720, Discriminator Loss: 0.42960016801953316, Generator Loss: 4.144633769989014\n",
            "Epoch: 730, Discriminator Loss: 0.3867647498846054, Generator Loss: 3.133229970932007\n",
            "Epoch: 740, Discriminator Loss: 4.660107761621475, Generator Loss: 0.05294383317232132\n",
            "Epoch: 750, Discriminator Loss: 0.7508412003517151, Generator Loss: 1.970673680305481\n",
            "Epoch: 760, Discriminator Loss: 0.35726370110933203, Generator Loss: 8.644185066223145\n",
            "Epoch: 770, Discriminator Loss: 4.25028195977211, Generator Loss: 0.046398527920246124\n",
            "Epoch: 780, Discriminator Loss: 0.5620723813772202, Generator Loss: 2.3336904048919678\n",
            "Epoch: 790, Discriminator Loss: 0.481756966561079, Generator Loss: 3.880925416946411\n",
            "Epoch: 800, Discriminator Loss: 0.454071503598243, Generator Loss: 5.893301010131836\n",
            "Epoch: 810, Discriminator Loss: 0.7503899931907654, Generator Loss: 1.4426778554916382\n",
            "Epoch: 820, Discriminator Loss: 2.7681212425231934, Generator Loss: 0.19290877878665924\n",
            "Epoch: 830, Discriminator Loss: 3.251029521226883, Generator Loss: 0.09127476811408997\n",
            "Epoch: 840, Discriminator Loss: 1.7040368616580963, Generator Loss: 0.5106686353683472\n",
            "Epoch: 850, Discriminator Loss: 7.62799933552742, Generator Loss: 0.0013835460413247347\n",
            "Epoch: 860, Discriminator Loss: 0.34062111750245094, Generator Loss: 5.557577610015869\n",
            "Epoch: 870, Discriminator Loss: 0.29741517748334445, Generator Loss: 8.451079368591309\n",
            "Epoch: 880, Discriminator Loss: 0.2212230113800615, Generator Loss: 7.125008583068848\n",
            "Epoch: 890, Discriminator Loss: 3.8669199645519257, Generator Loss: 0.0663108080625534\n",
            "Epoch: 900, Discriminator Loss: 0.29732700577005744, Generator Loss: 5.8115692138671875\n",
            "Epoch: 910, Discriminator Loss: 0.29090526327490807, Generator Loss: 4.492821216583252\n",
            "Epoch: 920, Discriminator Loss: 0.388845294713974, Generator Loss: 3.0015323162078857\n",
            "Epoch: 930, Discriminator Loss: 0.3724579065346916, Generator Loss: 11.352917671203613\n",
            "Epoch: 940, Discriminator Loss: 0.30348015803974704, Generator Loss: 10.405496597290039\n",
            "Epoch: 950, Discriminator Loss: 0.6707999408245087, Generator Loss: 1.6518282890319824\n",
            "Epoch: 960, Discriminator Loss: 0.6311497986316681, Generator Loss: 1.5500314235687256\n",
            "Epoch: 970, Discriminator Loss: 0.2534630484879017, Generator Loss: 4.9596147537231445\n",
            "Epoch: 980, Discriminator Loss: 1.1982026100158691, Generator Loss: 0.9539681673049927\n",
            "Epoch: 990, Discriminator Loss: 0.6875620186328888, Generator Loss: 1.6224805116653442\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SwarmAgent:\n",
        "    def __init__(self, agent_id, position):\n",
        "        self.agent_id = agent_id\n",
        "        self.position = position\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "class AIRL:\n",
        "    def __init__(self, num_agents, environment_size, expert_trajectories, num_epochs, learning_rate):\n",
        "        self.num_agents = num_agents\n",
        "        self.environment_size = environment_size\n",
        "        self.expert_trajectories = expert_trajectories\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.discriminator = Discriminator(environment_size[0] * environment_size[1])\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.Linear(environment_size[0] * environment_size[1], 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, environment_size[0] * environment_size[1]),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.optimizer_d = optim.Adam(self.discriminator.parameters(), lr=learning_rate)\n",
        "        self.optimizer_g = optim.Adam(self.generator.parameters(), lr=learning_rate)\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def train(self):\n",
        "        expert_states = self.get_state_trajectories(self.expert_trajectories)\n",
        "        expert_labels = torch.ones(len(expert_states)).unsqueeze(1)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.optimizer_d.zero_grad()\n",
        "\n",
        "            # Train discriminator with expert data\n",
        "            expert_output = self.discriminator(expert_states)\n",
        "            loss_expert = self.criterion(expert_output, expert_labels)\n",
        "            loss_expert.backward()\n",
        "\n",
        "            # Train discriminator with generated data\n",
        "            generated_states = self.generate_states(len(expert_states))\n",
        "            generated_labels = torch.zeros(len(generated_states)).unsqueeze(1)\n",
        "            generated_output = self.discriminator(generated_states.detach())\n",
        "            loss_generated = self.criterion(generated_output, generated_labels)\n",
        "            loss_generated.backward()\n",
        "\n",
        "            self.optimizer_d.step()\n",
        "\n",
        "            # Train generator\n",
        "            self.optimizer_g.zero_grad()\n",
        "            generated_output = self.discriminator(generated_states)\n",
        "            loss_generator = self.criterion(generated_output, expert_labels)\n",
        "            loss_generator.backward()\n",
        "            self.optimizer_g.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch: {epoch}, Discriminator Loss: {loss_expert.item() + loss_generated.item()}, Generator Loss: {loss_generator.item()}\")\n",
        "\n",
        "    def get_state_trajectories(self, trajectories):\n",
        "        states = []\n",
        "\n",
        "        for trajectory in trajectories:\n",
        "            agent_positions = [agent.position for agent in trajectory]\n",
        "            state = np.zeros(self.environment_size)\n",
        "            for agent_position in agent_positions:\n",
        "                state[tuple(agent_position)] += 1\n",
        "            states.append(state.flatten())\n",
        "\n",
        "        return torch.tensor(states, dtype=torch.float)\n",
        "\n",
        "    def generate_states(self, num_states):\n",
        "        noise = torch.randn(num_states, self.environment_size[0] * self.environment_size[1])\n",
        "        generated_states = self.generator(noise)\n",
        "        return generated_states\n",
        "\n",
        "# Usage example\n",
        "num_agents = 10\n",
        "environment_size = (10, 10)\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "swarm_agents = []\n",
        "for i in range(num_agents):\n",
        "    position = np.random.randint(0, environment_size[0]), np.random.randint(0, environment_size[1])\n",
        "    agent = SwarmAgent(i, position)\n",
        "    swarm_agents.append(agent)\n",
        "\n",
        "expert_trajectories = [swarm_agents]  # List of expert agent trajectories\n",
        "\n",
        "airl = AIRL(num_agents, environment_size, expert_trajectories, num_epochs, learning_rate)\n",
        "airl.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code starts by defining a SwarmAgent class representing an agent in the swarm. Each agent has an agent ID and a position in the environment.\n",
        "\n",
        "The Discriminator class is defined as a neural network with two fully connected layers and a sigmoid activation function.\n",
        "\n",
        "The AIRL class represents the Adversarial Inverse Reinforcement Learning algorithm. It takes the number of agents, environment size, expert trajectories, number of training epochs, and learning rate as input.\n",
        "\n",
        "The AIRL class contains a discriminator and a generator. The discriminator estimates the reward function, and the generator generates states.\n",
        "\n",
        "The train method trains the discriminator and generator using the expert trajectories. It iterates for the specified number of epochs.\n",
        "\n",
        "The discriminator is trained using both expert and generated data. Expert trajectories are labeled as 1, and generated trajectories are labeled as 0.\n",
        "\n",
        "The generator is trained to generate states that fool the discriminator by minimizing the generator loss.\n",
        "\n",
        "The get_state_trajectories method converts agent trajectories into state trajectories. Each state is represented by a flattened array, where each element represents the density of agents in that position.\n",
        "\n",
        "The generate_states method generates states using the generator model.\n",
        "\n",
        "In the usage example, a swarm of agents is created with random positions in the environment.\n",
        "\n",
        "Expert trajectories are defined as a list of agent trajectories. In this example, only a single expert trajectory is used.\n",
        "\n",
        "An instance of AIRL is created, passing the necessary parameters.\n",
        "\n",
        "The train method is called to train the discriminator and generator."
      ],
      "metadata": {
        "id": "_KZTVnZwbdUd"
      }
    }
  ]
}