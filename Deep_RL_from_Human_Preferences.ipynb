{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ClvBAK-cfbM",
        "outputId": "0685f43e-ac0e-4e6c-a889-3c5c341a1f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-3526cadb60d5>:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  return torch.tensor(states, dtype=torch.float), torch.tensor(actions)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.286588430404663\n",
            "Epoch: 10, Loss: 2.190488815307617\n",
            "Epoch: 20, Loss: 2.0010759830474854\n",
            "Epoch: 30, Loss: 1.7105687856674194\n",
            "Epoch: 40, Loss: 1.5266937017440796\n",
            "Epoch: 50, Loss: 1.4800370931625366\n",
            "Epoch: 60, Loss: 1.4696131944656372\n",
            "Epoch: 70, Loss: 1.4664384126663208\n",
            "Epoch: 80, Loss: 1.465113639831543\n",
            "Epoch: 90, Loss: 1.4644008874893188\n",
            "Epoch: 100, Loss: 1.4639370441436768\n",
            "Epoch: 110, Loss: 1.4635950326919556\n",
            "Epoch: 120, Loss: 1.463323712348938\n",
            "Epoch: 130, Loss: 1.4630993604660034\n",
            "Epoch: 140, Loss: 1.4629099369049072\n",
            "Epoch: 150, Loss: 1.4627474546432495\n",
            "Epoch: 160, Loss: 1.4626070261001587\n",
            "Epoch: 170, Loss: 1.46248459815979\n",
            "Epoch: 180, Loss: 1.4623771905899048\n",
            "Epoch: 190, Loss: 1.4622825384140015\n",
            "Epoch: 200, Loss: 1.4621984958648682\n",
            "Epoch: 210, Loss: 1.4621235132217407\n",
            "Epoch: 220, Loss: 1.4620563983917236\n",
            "Epoch: 230, Loss: 1.4619964361190796\n",
            "Epoch: 240, Loss: 1.4619420766830444\n",
            "Epoch: 250, Loss: 1.46189284324646\n",
            "Epoch: 260, Loss: 1.4618483781814575\n",
            "Epoch: 270, Loss: 1.4618076086044312\n",
            "Epoch: 280, Loss: 1.4617705345153809\n",
            "Epoch: 290, Loss: 1.4617366790771484\n",
            "Epoch: 300, Loss: 1.4617053270339966\n",
            "Epoch: 310, Loss: 1.4616765975952148\n",
            "Epoch: 320, Loss: 1.461649775505066\n",
            "Epoch: 330, Loss: 1.4616254568099976\n",
            "Epoch: 340, Loss: 1.4616026878356934\n",
            "Epoch: 350, Loss: 1.4615814685821533\n",
            "Epoch: 360, Loss: 1.461561918258667\n",
            "Epoch: 370, Loss: 1.4615434408187866\n",
            "Epoch: 380, Loss: 1.4615263938903809\n",
            "Epoch: 390, Loss: 1.461510419845581\n",
            "Epoch: 400, Loss: 1.461495280265808\n",
            "Epoch: 410, Loss: 1.4614813327789307\n",
            "Epoch: 420, Loss: 1.4614681005477905\n",
            "Epoch: 430, Loss: 1.4614558219909668\n",
            "Epoch: 440, Loss: 1.4614439010620117\n",
            "Epoch: 450, Loss: 1.4614330530166626\n",
            "Epoch: 460, Loss: 1.4614226818084717\n",
            "Epoch: 470, Loss: 1.461412787437439\n",
            "Epoch: 480, Loss: 1.4614036083221436\n",
            "Epoch: 490, Loss: 1.4613946676254272\n",
            "Epoch: 500, Loss: 1.4613862037658691\n",
            "Epoch: 510, Loss: 1.4613782167434692\n",
            "Epoch: 520, Loss: 1.4613707065582275\n",
            "Epoch: 530, Loss: 1.461363673210144\n",
            "Epoch: 540, Loss: 1.46135675907135\n",
            "Epoch: 550, Loss: 1.4613502025604248\n",
            "Epoch: 560, Loss: 1.4613441228866577\n",
            "Epoch: 570, Loss: 1.4613381624221802\n",
            "Epoch: 580, Loss: 1.4613325595855713\n",
            "Epoch: 590, Loss: 1.461327075958252\n",
            "Epoch: 600, Loss: 1.4613219499588013\n",
            "Epoch: 610, Loss: 1.4613170623779297\n",
            "Epoch: 620, Loss: 1.4613122940063477\n",
            "Epoch: 630, Loss: 1.4613078832626343\n",
            "Epoch: 640, Loss: 1.461303472518921\n",
            "Epoch: 650, Loss: 1.4612994194030762\n",
            "Epoch: 660, Loss: 1.461295247077942\n",
            "Epoch: 670, Loss: 1.4612915515899658\n",
            "Epoch: 680, Loss: 1.4612877368927002\n",
            "Epoch: 690, Loss: 1.4612842798233032\n",
            "Epoch: 700, Loss: 1.4612809419631958\n",
            "Epoch: 710, Loss: 1.4612776041030884\n",
            "Epoch: 720, Loss: 1.46127450466156\n",
            "Epoch: 730, Loss: 1.4612712860107422\n",
            "Epoch: 740, Loss: 1.4612685441970825\n",
            "Epoch: 750, Loss: 1.4612656831741333\n",
            "Epoch: 760, Loss: 1.4612629413604736\n",
            "Epoch: 770, Loss: 1.4612600803375244\n",
            "Epoch: 780, Loss: 1.4612575769424438\n",
            "Epoch: 790, Loss: 1.4612553119659424\n",
            "Epoch: 800, Loss: 1.4612529277801514\n",
            "Epoch: 810, Loss: 1.46125066280365\n",
            "Epoch: 820, Loss: 1.4612483978271484\n",
            "Epoch: 830, Loss: 1.461246132850647\n",
            "Epoch: 840, Loss: 1.4612442255020142\n",
            "Epoch: 850, Loss: 1.4612421989440918\n",
            "Epoch: 860, Loss: 1.4612401723861694\n",
            "Epoch: 870, Loss: 1.4612382650375366\n",
            "Epoch: 880, Loss: 1.461236596107483\n",
            "Epoch: 890, Loss: 1.46123468875885\n",
            "Epoch: 900, Loss: 1.461233139038086\n",
            "Epoch: 910, Loss: 1.4612313508987427\n",
            "Epoch: 920, Loss: 1.4612295627593994\n",
            "Epoch: 930, Loss: 1.4612280130386353\n",
            "Epoch: 940, Loss: 1.4612267017364502\n",
            "Epoch: 950, Loss: 1.4612250328063965\n",
            "Epoch: 960, Loss: 1.4612236022949219\n",
            "Epoch: 970, Loss: 1.4612221717834473\n",
            "Epoch: 980, Loss: 1.4612209796905518\n",
            "Epoch: 990, Loss: 1.4612196683883667\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3526cadb60d5>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Test the policy by selecting actions for agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdrlhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mswarm_agents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actions:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3526cadb60d5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Test the policy by selecting actions for agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdrlhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mswarm_agents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actions:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3526cadb60d5>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3526cadb60d5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x10 and 100x64)"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SwarmAgent:\n",
        "    def __init__(self, agent_id, position):\n",
        "        self.agent_id = agent_id\n",
        "        self.position = position\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "class DRLHP:\n",
        "    def __init__(self, num_agents, environment_size, num_epochs, learning_rate):\n",
        "        self.num_agents = num_agents\n",
        "        self.environment_size = environment_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.policy_network = PolicyNetwork(environment_size[0] * environment_size[1], num_agents)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self, expert_preferences):\n",
        "        expert_states, expert_actions = self.get_state_action_preferences(expert_preferences)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            predicted_actions = self.policy_network(expert_states)\n",
        "            loss = self.criterion(predicted_actions, expert_actions)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    def get_state_action_preferences(self, preferences):\n",
        "        states = []\n",
        "        actions = []\n",
        "\n",
        "        for preference in preferences:\n",
        "            agent_positions = [agent.position for agent in preference['agents']]\n",
        "            state = np.zeros(self.environment_size)\n",
        "            for agent_position in agent_positions:\n",
        "                state[tuple(agent_position)] += 1\n",
        "            states.append(state.flatten())\n",
        "\n",
        "            preferred_agent_id = preference['preferred_agent'].agent_id\n",
        "            actions.append(preferred_agent_id)\n",
        "\n",
        "        return torch.tensor(states, dtype=torch.float), torch.tensor(actions)\n",
        "\n",
        "    def select_action(self, agent):\n",
        "        state = np.zeros(self.environment_size)\n",
        "        state[tuple(agent.position)] = 1\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action_probs = self.policy_network(state.unsqueeze(0))\n",
        "        action = torch.argmax(action_probs).item()\n",
        "        return action\n",
        "\n",
        "# Usage example\n",
        "num_agents = 10\n",
        "environment_size = (10, 10)\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "swarm_agents = []\n",
        "for i in range(num_agents):\n",
        "    position = np.random.randint(0, environment_size[0]), np.random.randint(0, environment_size[1])\n",
        "    agent = SwarmAgent(i, position)\n",
        "    swarm_agents.append(agent)\n",
        "\n",
        "expert_preferences = [\n",
        "    {'agents': swarm_agents, 'preferred_agent': swarm_agents[0]},  # Example preference\n",
        "    # Add more preferences as needed\n",
        "]\n",
        "\n",
        "drlhp = DRLHP(num_agents, environment_size, num_epochs, learning_rate)\n",
        "drlhp.train(expert_preferences)\n",
        "\n",
        "# Test the policy by selecting actions for agents\n",
        "actions = [drlhp.select_action(agent) for agent in swarm_agents]\n",
        "print(\"Actions:\", actions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "he code starts by defining a SwarmAgent class representing an agent in the swarm. Each agent has an agent ID and a position in the environment.\n",
        "\n",
        "The PolicyNetwork class is defined as a neural network with two fully connected layers and a softmax activation function.\n",
        "\n",
        "The DRLHP class represents the Deep Reinforcement Learning from Human Preferences algorithm. It takes the number of agents, environment size, number of training epochs, and learning rate as input.\n",
        "\n",
        "The DRLHP class contains a policy network, an optimizer, and a loss criterion.\n",
        "\n",
        "The train method trains the policy network using expert preferences. It iterates for the specified number of epochs.\n",
        "\n",
        "The policy network is trained using the cross-entropy loss between predicted actions and expert actions.\n",
        "\n",
        "The get_state_action_preferences method extracts state-action pairs from expert preferences.\n",
        "\n",
        "The select_action method selects an action for a given agent using the trained policy network.\n",
        "\n",
        "In the usage example, a swarm of agents is created with random positions in the environment.\n",
        "\n",
        "Expert preferences are defined as a list of dictionaries, where each dictionary contains a list of agents and the preferred agent.\n",
        "\n",
        "An instance of DRLHP is created, passing the necessary parameters.\n",
        "\n",
        "The train method is called to train the policy network using expert preferences.\n",
        "\n",
        "The select_action method is called for each agent to select actions based on the learned policy.\n",
        "\n",
        "The selected actions for each agent are printed."
      ],
      "metadata": {
        "id": "sqdIjIC0dJK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SwarmAgent:\n",
        "    def __init__(self, agent_id, position):\n",
        "        self.agent_id = agent_id\n",
        "        self.position = position\n",
        "\n",
        "class DeepRLHP:\n",
        "    def __init__(self, num_agents, environment_size, num_iterations, learning_rate):\n",
        "        self.num_agents = num_agents\n",
        "        self.environment_size = environment_size\n",
        "        self.num_iterations = num_iterations\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.agent_network = nn.Sequential(\n",
        "            nn.Linear(environment_size[0] * environment_size[1], 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_agents)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.agent_network.parameters(), lr=learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def train(self, expert_preferences):\n",
        "        for _ in range(self.num_iterations):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Collect comparison-based training data\n",
        "            states, comparisons = self.collect_training_data(expert_preferences)\n",
        "\n",
        "            # Convert states to tensor\n",
        "            states = torch.tensor(states, dtype=torch.float)\n",
        "\n",
        "            # Predict preferences from the agent\n",
        "            predicted_preferences = self.agent_network(states)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.criterion(predicted_preferences, torch.tensor(comparisons, dtype=torch.float).unsqueeze(1))\n",
        "            loss.backward()\n",
        "\n",
        "            # Update agent network\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def collect_training_data(self, expert_preferences):\n",
        "        states = []\n",
        "        comparisons = []\n",
        "\n",
        "        for preference in expert_preferences:\n",
        "            state1 = np.zeros(self.environment_size)\n",
        "            state2 = np.zeros(self.environment_size)\n",
        "\n",
        "            agent1, agent2 = preference\n",
        "            state1[tuple(agent1.position)] += 1\n",
        "            state2[tuple(agent2.position)] += 1\n",
        "\n",
        "            states.append(state1.flatten())\n",
        "            states.append(state2.flatten())\n",
        "            comparisons.append(1.0)\n",
        "            comparisons.append(-1.0)\n",
        "\n",
        "        return states, comparisons\n",
        "\n",
        "    def get_action(self, agent):\n",
        "        state = np.zeros(self.environment_size)\n",
        "        state[tuple(agent.position)] += 1\n",
        "\n",
        "        state_tensor = torch.tensor(state.flatten(), dtype=torch.float)\n",
        "        action_tensor = self.agent_network(state_tensor)\n",
        "\n",
        "        action = torch.argmax(action_tensor).item()\n",
        "        return action\n",
        "\n",
        "# Usage example\n",
        "num_agents = 10\n",
        "environment_size = (10, 10)\n",
        "num_iterations = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "swarm_agents = []\n",
        "for i in range(num_agents):\n",
        "    position = np.random.randint(0, environment_size[0]), np.random.randint(0, environment_size[1])\n",
        "    agent = SwarmAgent(i, position)\n",
        "    swarm_agents.append(agent)\n",
        "\n",
        "expert_preferences = [(swarm_agents[0], swarm_agents[1]), (swarm_agents[2], swarm_agents[3])]  # List of expert preferences\n",
        "\n",
        "drlhp = DeepRLHP(num_agents, environment_size, num_iterations, learning_rate)\n",
        "drlhp.train(expert_preferences)\n",
        "\n",
        "# Get action for a specific agent\n",
        "agent_id = 0\n",
        "action = drlhp.get_action(swarm_agents[agent_id])\n",
        "print(f\"Action for Agent {agent_id}: {action}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLjEaXtNeOq8",
        "outputId": "7f9303cf-4488-47a2-e4ba-7e0f1daac6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4, 1])) that is different to the input size (torch.Size([4, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action for Agent 0: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code starts by defining a SwarmAgent class representing an agent in the swarm. Each agent has an agent ID and a position in the environment.\n",
        "\n",
        "The DeepRLHP class represents the Deep Reinforcement Learning from Human Preferences algorithm. It takes the number of agents, environment size, number of iterations, and learning rate as input.\n",
        "\n",
        "The DeepRLHP class contains an agent network, optimizer, and loss criterion. The agent network is a feedforward neural network.\n",
        "\n",
        "The agent network is defined with two linear layers. The first layer takes as input the flattened size of the environment and outputs 64 units, followed by a ReLU activation function. The second layer takes the 64 units as input and outputs a number of units equal to the number of agents.\n",
        "\n",
        "The train method trains the agent network using expert preferences. It iterates for the specified number of iterations.\n",
        "\n",
        "The collect_training_data method collects comparison-based training data from expert preferences. It generates two states corresponding to each preference and assigns a preference value of 1.0 or -1.0 based on the expert preference. The states and preferences are stored in separate lists.\n",
        "\n",
        "The get_action method retrieves the action for a specific agent based on its position. It first constructs a state representation by creating an environment-sized array and setting the position of the agent to 1.0. The state is then converted to a tensor and passed through the agent network. The action with the highest value is extracted using torch.argmax and converted to a Python scalar using .item().\n",
        "\n",
        "In the usage example, a swarm of agents is created with random positions in the environment.\n",
        "\n",
        "Expert preferences are defined as a list of tuples, where each tuple contains two agents representing a preference.\n",
        "\n",
        "An instance of DeepRLHP is created, passing the necessary parameters.\n",
        "\n",
        "The train method is called to train the agent network using the expert preferences.\n",
        "\n",
        "The get_action method is called to retrieve the action for a specific agent.\n",
        "\n",
        "The action for the agent is printed."
      ],
      "metadata": {
        "id": "g89x_yY2e6NQ"
      }
    }
  ]
}