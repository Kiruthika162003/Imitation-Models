{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DAggerAgent(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DAggerAgent, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train_dagger_agent(expert_data, num_epochs, num_iterations):\n",
        "    # Prepare expert data\n",
        "    expert_states, expert_actions = expert_data\n",
        "\n",
        "    # Initialize agent\n",
        "    input_size = expert_states.shape[1]\n",
        "    output_size = expert_actions.shape[1]\n",
        "    agent = DAggerAgent(input_size, output_size)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=0.001)\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        if iteration == 0:\n",
        "            # Collect initial agent's data using expert policy\n",
        "            agent_states = expert_states\n",
        "            agent_actions = expert_actions\n",
        "        else:\n",
        "            # Aggregate data from expert and agent\n",
        "            all_states = torch.cat([expert_states, agent_states])\n",
        "            all_actions = torch.cat([expert_actions, agent_actions])\n",
        "\n",
        "            # Training loop\n",
        "            for epoch in range(num_epochs):\n",
        "                optimizer.zero_grad()\n",
        "                outputs = agent(all_states)\n",
        "                loss = criterion(outputs, all_actions)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if (epoch+1) % 10 == 0:\n",
        "                    print('Iteration [{}/{}], Epoch [{}/{}], Loss: {:.4f}'.format(iteration+1, num_iterations, epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "            # Collect agent's new data using the current policy\n",
        "            agent_states, agent_actions = collect_agent_data(agent)\n",
        "\n",
        "    return agent\n",
        "\n",
        "def collect_agent_data(agent):\n",
        "    # Implement agent's behavior to collect data\n",
        "    # and return agent states and actions\n",
        "    # Placeholder implementation: Random actions\n",
        "    agent_states = torch.randn((3, 3))\n",
        "    agent_actions = torch.randn((3, 2))\n",
        "    return agent_states, agent_actions\n",
        "\n",
        "# Usage example\n",
        "expert_states = torch.tensor([[0.1, 0.2, 0.3],\n",
        "                              [0.4, 0.5, 0.6],\n",
        "                              [0.7, 0.8, 0.9]])\n",
        "expert_actions = torch.tensor([[0.3, 0.4],\n",
        "                               [0.5, 0.6],\n",
        "                               [0.7, 0.8]])\n",
        "\n",
        "num_epochs = 100\n",
        "num_iterations = 5\n",
        "agent = train_dagger_agent((expert_states, expert_actions), num_epochs, num_iterations)\n",
        "\n",
        "# Test the trained agent\n",
        "test_state = torch.tensor([[0.2, 0.3, 0.4]])\n",
        "action = agent(test_state)\n",
        "print('Action:', action)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAANj6qtSaQi",
        "outputId": "9b247ad1-8c1d-42b5-b342-17b7de8dce28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration [2/5], Epoch [10/100], Loss: 0.3832\n",
            "Iteration [2/5], Epoch [20/100], Loss: 0.1811\n",
            "Iteration [2/5], Epoch [30/100], Loss: 0.0659\n",
            "Iteration [2/5], Epoch [40/100], Loss: 0.0160\n",
            "Iteration [2/5], Epoch [50/100], Loss: 0.0024\n",
            "Iteration [2/5], Epoch [60/100], Loss: 0.0009\n",
            "Iteration [2/5], Epoch [70/100], Loss: 0.0008\n",
            "Iteration [2/5], Epoch [80/100], Loss: 0.0006\n",
            "Iteration [2/5], Epoch [90/100], Loss: 0.0003\n",
            "Iteration [2/5], Epoch [100/100], Loss: 0.0002\n",
            "Iteration [3/5], Epoch [10/100], Loss: 0.3069\n",
            "Iteration [3/5], Epoch [20/100], Loss: 0.2336\n",
            "Iteration [3/5], Epoch [30/100], Loss: 0.1875\n",
            "Iteration [3/5], Epoch [40/100], Loss: 0.1533\n",
            "Iteration [3/5], Epoch [50/100], Loss: 0.1244\n",
            "Iteration [3/5], Epoch [60/100], Loss: 0.1001\n",
            "Iteration [3/5], Epoch [70/100], Loss: 0.0802\n",
            "Iteration [3/5], Epoch [80/100], Loss: 0.0641\n",
            "Iteration [3/5], Epoch [90/100], Loss: 0.0516\n",
            "Iteration [3/5], Epoch [100/100], Loss: 0.0419\n",
            "Iteration [4/5], Epoch [10/100], Loss: 0.3219\n",
            "Iteration [4/5], Epoch [20/100], Loss: 0.2258\n",
            "Iteration [4/5], Epoch [30/100], Loss: 0.1540\n",
            "Iteration [4/5], Epoch [40/100], Loss: 0.1047\n",
            "Iteration [4/5], Epoch [50/100], Loss: 0.0722\n",
            "Iteration [4/5], Epoch [60/100], Loss: 0.0507\n",
            "Iteration [4/5], Epoch [70/100], Loss: 0.0363\n",
            "Iteration [4/5], Epoch [80/100], Loss: 0.0270\n",
            "Iteration [4/5], Epoch [90/100], Loss: 0.0207\n",
            "Iteration [4/5], Epoch [100/100], Loss: 0.0163\n",
            "Iteration [5/5], Epoch [10/100], Loss: 0.2026\n",
            "Iteration [5/5], Epoch [20/100], Loss: 0.1330\n",
            "Iteration [5/5], Epoch [30/100], Loss: 0.0949\n",
            "Iteration [5/5], Epoch [40/100], Loss: 0.0683\n",
            "Iteration [5/5], Epoch [50/100], Loss: 0.0508\n",
            "Iteration [5/5], Epoch [60/100], Loss: 0.0390\n",
            "Iteration [5/5], Epoch [70/100], Loss: 0.0312\n",
            "Iteration [5/5], Epoch [80/100], Loss: 0.0255\n",
            "Iteration [5/5], Epoch [90/100], Loss: 0.0210\n",
            "Iteration [5/5], Epoch [100/100], Loss: 0.0173\n",
            "Action: tensor([[0.2920, 0.4940]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6-LWfz_SpuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Certainly! Here's a simplified step-by-step explanation of how the provided code for DAgger in swarm multi-agent systems works:\n",
        "\n",
        "The code defines a neural network model called DAggerAgent that represents each agent in the swarm. It has two layers with a ReLU activation function.\n",
        "\n",
        "The train_dagger_agent function trains the DAgger agent using expert data. It takes expert states and actions, the number of training epochs per iteration, and the total number of iterations.\n",
        "\n",
        "Inside train_dagger_agent, the agent is initialized and the loss function and optimizer are defined.\n",
        "\n",
        "The training loop begins for each iteration:\n",
        "\n",
        "In the first iteration, the agent is trained using expert data.\n",
        "In subsequent iterations, the agent is trained using a combination of expert data and its own collected data.\n",
        "The agent's states and actions are aggregated from expert and agent data.\n",
        "The agent is trained for the specified number of epochs using the aggregated data.\n",
        "After each epoch, the agent collects new data using its current policy.\n",
        "After all iterations, the trained agent is returned from the train_dagger_agent function.\n",
        "\n",
        "In the usage example, expert data, the number of training epochs per iteration, and the total number of iterations are provided.\n",
        "\n",
        "The train_dagger_agent function is called with the expert data and training parameters to train the DAgger agent.\n",
        "\n",
        "After training, the trained agent can be tested by providing a test state. The agent predicts the corresponding action for the test state.\n",
        "\n",
        "Finally, the predicted action is printed.\n",
        "\n",
        " the provided collect_agent_data function is a placeholder that generates random data and should be replaced with the actual behavior of the agent to collect data in your specific swarm multi-agent scenario."
      ],
      "metadata": {
        "id": "GAmSZ7nBVO5U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IVPK9_bhV5h7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}